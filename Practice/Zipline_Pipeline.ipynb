{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipline Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "On any given trading day, the entire universe of stocks consists of thousands of securities. Usually, you will not be  interested in investing in all the stocks in the entire universe, but rather, you will likely select only a subset of these to invest. For example, you may only want to invest in stocks that have a 10-day average closing price of \\$10.00 or less. Or you may only want to invest in the top 500 securities ranked by some factor.\n",
    "\n",
    "In order to avoid spending a lot of time doing data wrangling to select only the securities you are interested in, people often use **pipelines**. In general, a pipeline is a placeholder for a series of data operations used to filter and rank data according to some factor or factors. \n",
    "\n",
    "In this notebook, you will learn how to work with the **Zipline Pipeline**. Zipline is an open-source algorithmic trading simulator developed by *Quantopian*. We will learn how to use the Zipline Pipeline to filter stock data according to factors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data with Zipline\n",
    "\n",
    "Before we build our pipeline with Zipline, we will first see how we can load the stock data we are going to use into Zipline. Zipline uses **Data Bundles** to make it easy to use different data sources. A data bundle is a collection of pricing data, adjustment data, and an asset database. Zipline employs data bundles to preload data used to run backtests and store data for future runs. Zipline comes with a few data bundles by default but it also has the ability to ingest new bundles. The first step to using a data bundle is to ingest the data. Zipline's ingestion process will start by downloading the data or by loading data files from your local machine. It will then pass the data to a set of writer objects that converts the original data to Zipline’s internal format (`bcolz` for pricing data, and `SQLite` for split/merger/dividend data) that hs been optimized for speed. This new data is written to a standard location that Zipline can find. By default, the new data is written to a subdirectory of `ZIPLINE_ROOT/data/<bundle>`, where `<bundle>` is the name given to the bundle ingested and the subdirectory is named with the current date. This allows Zipline to look at older data and run backtests on older copies of the data. Running a backtest with an old ingestion makes it easier to reproduce backtest results later. \n",
    "\n",
    "In this notebook, we will be using stock data from **Quotemedia**. In the Udacity Workspace you will find that the stock data from Quotemedia has already been ingested into Zipline. Therefore, in the code below we will use Zipline's `bundles.load()` function to load our previously ingested stock data from Quotemedia. In order to use the `bundles.load()` function we first need to do a couple of things. First, we need to specify the name of the bundle previously ingested. In this case, the name of the Quotemedia data bundle is `eod-quotemedia`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/495 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import yfinance as yf \n",
    "import tqdm\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/rbpal/01-qt-project-03-smart-beta-portfolio-optimization/main/eod-quotemedia.csv',index_col=False)\n",
    "\n",
    "df.to_csv('../Data/eod-quotemedia.csv',index=False)\n",
    "\n",
    "# Convert 'date' column to datetime if it's not already\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Group by ticker and process each group\n",
    "for ticker, group in tqdm.tqdm(df.groupby('ticker')):\n",
    "    # Ensure 'date' is sorted and in datetime format\n",
    "    group = group.sort_values('date')\n",
    "    \n",
    "    # Get the start and end dates for this ticker\n",
    "    start_date = group['date'].min()\n",
    "    end_date = group['date'].max()\n",
    "    \n",
    "    # Download data from yfinance\n",
    "    dat = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "    \n",
    "    dat.columns = ['open','high','low','close','adj close','volume']\n",
    "    \n",
    "    # rename index \n",
    "    dat.index.name = 'date'\n",
    "    # Write data to a CSV file with the ticker name\n",
    "    dat.to_csv(f'../Data/data/eod-quotemedia/daily/{ticker}.csv',index=True)\n",
    "\n",
    "    break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>30.793991</td>\n",
       "      <td>31.309013</td>\n",
       "      <td>30.693848</td>\n",
       "      <td>31.180258</td>\n",
       "      <td>28.410973</td>\n",
       "      <td>5988473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-07-02</td>\n",
       "      <td>31.115879</td>\n",
       "      <td>31.394850</td>\n",
       "      <td>30.722462</td>\n",
       "      <td>30.822603</td>\n",
       "      <td>28.085075</td>\n",
       "      <td>4175127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-07-03</td>\n",
       "      <td>30.629471</td>\n",
       "      <td>31.087269</td>\n",
       "      <td>30.557940</td>\n",
       "      <td>30.879827</td>\n",
       "      <td>28.137220</td>\n",
       "      <td>2712959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>31.087269</td>\n",
       "      <td>31.680973</td>\n",
       "      <td>31.044350</td>\n",
       "      <td>31.638054</td>\n",
       "      <td>28.828104</td>\n",
       "      <td>2932724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-07-08</td>\n",
       "      <td>31.745352</td>\n",
       "      <td>31.952789</td>\n",
       "      <td>31.559372</td>\n",
       "      <td>31.731045</td>\n",
       "      <td>28.912830</td>\n",
       "      <td>3593000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>60.590000</td>\n",
       "      <td>60.590000</td>\n",
       "      <td>59.500000</td>\n",
       "      <td>59.770000</td>\n",
       "      <td>56.573132</td>\n",
       "      <td>2879800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>2017-06-26</td>\n",
       "      <td>59.790001</td>\n",
       "      <td>59.950001</td>\n",
       "      <td>59.150002</td>\n",
       "      <td>59.240002</td>\n",
       "      <td>56.071480</td>\n",
       "      <td>1535500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>2017-06-27</td>\n",
       "      <td>59.130001</td>\n",
       "      <td>59.570000</td>\n",
       "      <td>58.840000</td>\n",
       "      <td>58.880001</td>\n",
       "      <td>55.730732</td>\n",
       "      <td>1060300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>2017-06-28</td>\n",
       "      <td>59.270000</td>\n",
       "      <td>59.700001</td>\n",
       "      <td>59.200001</td>\n",
       "      <td>59.400002</td>\n",
       "      <td>56.222919</td>\n",
       "      <td>1191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>2017-06-29</td>\n",
       "      <td>59.349998</td>\n",
       "      <td>59.630001</td>\n",
       "      <td>58.490002</td>\n",
       "      <td>58.799999</td>\n",
       "      <td>55.778961</td>\n",
       "      <td>1356800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1008 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date       open       high        low      close  adj close  \\\n",
       "0     2013-07-01  30.793991  31.309013  30.693848  31.180258  28.410973   \n",
       "1     2013-07-02  31.115879  31.394850  30.722462  30.822603  28.085075   \n",
       "2     2013-07-03  30.629471  31.087269  30.557940  30.879827  28.137220   \n",
       "3     2013-07-05  31.087269  31.680973  31.044350  31.638054  28.828104   \n",
       "4     2013-07-08  31.745352  31.952789  31.559372  31.731045  28.912830   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "1003  2017-06-23  60.590000  60.590000  59.500000  59.770000  56.573132   \n",
       "1004  2017-06-26  59.790001  59.950001  59.150002  59.240002  56.071480   \n",
       "1005  2017-06-27  59.130001  59.570000  58.840000  58.880001  55.730732   \n",
       "1006  2017-06-28  59.270000  59.700001  59.200001  59.400002  56.222919   \n",
       "1007  2017-06-29  59.349998  59.630001  58.490002  58.799999  55.778961   \n",
       "\n",
       "       volume  \n",
       "0     5988473  \n",
       "1     4175127  \n",
       "2     2712959  \n",
       "3     2932724  \n",
       "4     3593000  \n",
       "...       ...  \n",
       "1003  2879800  \n",
       "1004  1535500  \n",
       "1005  1060300  \n",
       "1006  1191500  \n",
       "1007  1356800  \n",
       "\n",
       "[1008 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dat = pd.read_csv(f'../Data/data/eod-quotemedia/daily/{ticker}.csv')\n",
    "\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the bundle name\n",
    "bundle_name = 'eod-quotemedia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we need to register the data bundle and its ingest function with Zipline, using the `bundles.register()` function. The ingest function is responsible for loading the data into memory and passing it to a set of writer objects provided by Zipline to convert the data to Zipline’s internal format. Since the original Quotemedia data was contained in `.csv` files, we will use the `csvdir_equities()` function to generate the ingest function for our Quotemedia data bundle. In addition, since Quotemedia's `.csv` files contained daily stock data, we will set the time frame for our ingest function, to `daily`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CSVDIRBundle.ingest of <zipline.data.bundles.csvdir.CSVDIRBundle object at 0x7f9b02fc7790>>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from zipline.data import bundles\n",
    "from zipline.data.bundles.csvdir import csvdir_equities\n",
    "\n",
    "# Create an ingest function \n",
    "ingest_func = csvdir_equities(['daily'], bundle_name)\n",
    "\n",
    "# Register the data bundle and its ingest function\n",
    "bundles.register(bundle_name, ingest_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our data bundle and ingest function are registered, we can load our data using the `bundles.load()` function. Since this function loads our previously ingested data, we need to set `ZIPLINE_ROOT` to the path of the most recent ingested data. The most recent data is located in the `cwd/../../data/project_4_eod/` directory, where `cwd` is the current working directory. We will specify this location using the `os.environ[]` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23007/1887213284.py:15: UserWarning: Overwriting bundle with name 'eod-quotemedia'\n",
      "  bundles.register(bundle_name, ingest_func)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "no data for bundle 'eod-quotemedia' on or before 2024-09-02 17:30:01.296629+00:00\nmaybe you need to run: $ zipline ingest -b eod-quotemedia",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDateParseError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/zipline/data/bundles/core.py:494\u001b[0m, in \u001b[0;36m_make_bundle_core.<locals>.most_recent_data\u001b[0;34m(bundle_name, timestamp, environ)\u001b[0m\n\u001b[1;32m    488\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\n\u001b[1;32m    489\u001b[0m         pth\u001b[38;5;241m.\u001b[39mdata_path([bundle_name], environ\u001b[38;5;241m=\u001b[39menviron),\n\u001b[1;32m    490\u001b[0m     )\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pth\u001b[38;5;241m.\u001b[39mdata_path(\n\u001b[1;32m    492\u001b[0m         [\n\u001b[1;32m    493\u001b[0m             bundle_name,\n\u001b[0;32m--> 494\u001b[0m             \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcomplement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m                \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_bundle_ingest_dirname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    498\u001b[0m         ],\n\u001b[1;32m    499\u001b[0m         environ\u001b[38;5;241m=\u001b[39menviron,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/zipline/data/bundles/core.py:121\u001b[0m, in \u001b[0;36mfrom_bundle_ingest_dirname\u001b[0;34m(cs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read a bundle ingestion directory name into a pandas Timestamp.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    The time when this ingestion happened.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTimestamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mtimestamps.pyx:1865\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timestamps.Timestamp.__new__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mconversion.pyx:364\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.conversion.convert_to_tsobject\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mconversion.pyx:641\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.conversion.convert_str_to_tsobject\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsing.pyx:336\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsing.pyx:666\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDateParseError\u001b[0m: Unknown datetime string format, unable to parse: daily",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m bundles\u001b[38;5;241m.\u001b[39mingest(bundle_name)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Load the data bundle\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m bundle_data \u001b[38;5;241m=\u001b[39m \u001b[43mbundles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbundle_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/zipline/data/bundles/core.py:532\u001b[0m, in \u001b[0;36m_make_bundle_core.<locals>.load\u001b[0;34m(name, environ, timestamp)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timestamp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    531\u001b[0m     timestamp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimestamp\u001b[38;5;241m.\u001b[39mutcnow()\n\u001b[0;32m--> 532\u001b[0m timestr \u001b[38;5;241m=\u001b[39m \u001b[43mmost_recent_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BundleData(\n\u001b[1;32m    534\u001b[0m     asset_finder\u001b[38;5;241m=\u001b[39mAssetFinder(\n\u001b[1;32m    535\u001b[0m         asset_db_path(name, timestr, environ\u001b[38;5;241m=\u001b[39menviron),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m     ),\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/zipline/data/bundles/core.py:504\u001b[0m, in \u001b[0;36m_make_bundle_core.<locals>.most_recent_data\u001b[0;34m(bundle_name, timestamp, environ)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m, errno\u001b[38;5;241m.\u001b[39mENOENT) \u001b[38;5;241m!=\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOENT:\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 504\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno data for bundle \u001b[39m\u001b[38;5;132;01m{bundle!r}\u001b[39;00m\u001b[38;5;124m on or before \u001b[39m\u001b[38;5;132;01m{timestamp}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaybe you need to run: $ zipline ingest -b \u001b[39m\u001b[38;5;132;01m{bundle}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    507\u001b[0m         bundle\u001b[38;5;241m=\u001b[39mbundle_name,\n\u001b[1;32m    508\u001b[0m         timestamp\u001b[38;5;241m=\u001b[39mtimestamp,\n\u001b[1;32m    509\u001b[0m     ),\n\u001b[1;32m    510\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: no data for bundle 'eod-quotemedia' on or before 2024-09-02 17:30:01.296629+00:00\nmaybe you need to run: $ zipline ingest -b eod-quotemedia"
     ]
    }
   ],
   "source": [
    "# Specify the bundle name\n",
    "bundle_name = 'eod-quotemedia'\n",
    "\n",
    "from zipline.data import bundles\n",
    "from zipline.data.bundles.csvdir import csvdir_equities\n",
    "import os\n",
    "\n",
    "# Set environment variable 'ZIPLINE_ROOT' to the directory containing the data folder\n",
    "os.environ['ZIPLINE_ROOT'] = os.path.abspath('../Data/')\n",
    "\n",
    "# Create an ingest function for the 'daily' frequency data\n",
    "ingest_func = csvdir_equities(['daily'], os.path.join(os.environ['ZIPLINE_ROOT'], 'data', bundle_name))\n",
    "\n",
    "# Register the data bundle and its ingest function\n",
    "bundles.register(bundle_name, ingest_func)\n",
    "\n",
    "\n",
    "# Ingest the data bundle\n",
    "bundles.ingest(bundle_name)\n",
    "\n",
    "# Load the data bundle\n",
    "bundle_data = bundles.load(bundle_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "no data for bundle 'eod-quotemedia' on or before 2024-09-02 16:44:34.690247+00:00\nmaybe you need to run: $ zipline ingest -b eod-quotemedia",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDateParseError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/zipline/data/bundles/core.py:494\u001b[0m, in \u001b[0;36m_make_bundle_core.<locals>.most_recent_data\u001b[0;34m(bundle_name, timestamp, environ)\u001b[0m\n\u001b[1;32m    488\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\n\u001b[1;32m    489\u001b[0m         pth\u001b[38;5;241m.\u001b[39mdata_path([bundle_name], environ\u001b[38;5;241m=\u001b[39menviron),\n\u001b[1;32m    490\u001b[0m     )\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pth\u001b[38;5;241m.\u001b[39mdata_path(\n\u001b[1;32m    492\u001b[0m         [\n\u001b[1;32m    493\u001b[0m             bundle_name,\n\u001b[0;32m--> 494\u001b[0m             \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcomplement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m                \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_bundle_ingest_dirname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    498\u001b[0m         ],\n\u001b[1;32m    499\u001b[0m         environ\u001b[38;5;241m=\u001b[39menviron,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/zipline/data/bundles/core.py:121\u001b[0m, in \u001b[0;36mfrom_bundle_ingest_dirname\u001b[0;34m(cs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read a bundle ingestion directory name into a pandas Timestamp.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    The time when this ingestion happened.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTimestamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mtimestamps.pyx:1865\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timestamps.Timestamp.__new__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mconversion.pyx:364\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.conversion.convert_to_tsobject\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mconversion.pyx:641\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.conversion.convert_str_to_tsobject\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsing.pyx:336\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsing.pyx:666\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDateParseError\u001b[0m: Unknown datetime string format, unable to parse: daily",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZIPLINE_ROOT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Data/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the data bundle\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m bundle_data \u001b[38;5;241m=\u001b[39m \u001b[43mbundles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbundle_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/zipline/data/bundles/core.py:532\u001b[0m, in \u001b[0;36m_make_bundle_core.<locals>.load\u001b[0;34m(name, environ, timestamp)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timestamp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    531\u001b[0m     timestamp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimestamp\u001b[38;5;241m.\u001b[39mutcnow()\n\u001b[0;32m--> 532\u001b[0m timestr \u001b[38;5;241m=\u001b[39m \u001b[43mmost_recent_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BundleData(\n\u001b[1;32m    534\u001b[0m     asset_finder\u001b[38;5;241m=\u001b[39mAssetFinder(\n\u001b[1;32m    535\u001b[0m         asset_db_path(name, timestr, environ\u001b[38;5;241m=\u001b[39menviron),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m     ),\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/zipline/data/bundles/core.py:504\u001b[0m, in \u001b[0;36m_make_bundle_core.<locals>.most_recent_data\u001b[0;34m(bundle_name, timestamp, environ)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m, errno\u001b[38;5;241m.\u001b[39mENOENT) \u001b[38;5;241m!=\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOENT:\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 504\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno data for bundle \u001b[39m\u001b[38;5;132;01m{bundle!r}\u001b[39;00m\u001b[38;5;124m on or before \u001b[39m\u001b[38;5;132;01m{timestamp}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaybe you need to run: $ zipline ingest -b \u001b[39m\u001b[38;5;132;01m{bundle}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    507\u001b[0m         bundle\u001b[38;5;241m=\u001b[39mbundle_name,\n\u001b[1;32m    508\u001b[0m         timestamp\u001b[38;5;241m=\u001b[39mtimestamp,\n\u001b[1;32m    509\u001b[0m     ),\n\u001b[1;32m    510\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: no data for bundle 'eod-quotemedia' on or before 2024-09-02 16:44:34.690247+00:00\nmaybe you need to run: $ zipline ingest -b eod-quotemedia"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variable 'ZIPLINE_ROOT' to the path where the most recent data is located\n",
    "os.environ['ZIPLINE_ROOT'] = '../Data/'\n",
    "\n",
    "# Load the data bundle\n",
    "bundle_data = bundles.load(bundle_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Empty Pipeline\n",
    "\n",
    "Once we have loaded our data, we can start building our Zipline pipeline. We begin by creating an empty Pipeline object using Zipline's `Pipeline` class. A Pipeline object represents a collection of named expressions to be compiled and executed by a Pipeline Engine. The `Pipeline(columns=None, screen=None)` class takes two optional parameters, `columns` and `screen`. The `columns` parameter is a dictionary used to indicate the intial columns to use, and the `screen` parameter is used to setup a screen to exclude unwanted data. \n",
    "\n",
    "In the code below we will create a `screen` for our pipeline using Zipline's built-in `.AverageDollarVolume()` class. We will use the `.AverageDollarVolume()` class to produce a 60-day Average Dollar Volume of closing prices for every stock in our universe. We then use the `.top(10)` attribute to specify that we want to filter down our universe each day to just the top 10 assets. Therefore, this screen will act as a filter to exclude data from our stock universe each day. The average dollar volume is a good first pass filter to avoid illiquid assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline import Pipeline\n",
    "from zipline.pipeline.factors import AverageDollarVolume\n",
    "\n",
    "# Create a screen for our Pipeline\n",
    "universe = AverageDollarVolume(window_length = 60).top(10)\n",
    "\n",
    "# Create an empty Pipeline with the given screen\n",
    "pipeline = Pipeline(screen = universe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above we have named our Pipeline object `pipeline` so that we can identify it later when we make computations. Remember a Pipeline is an object that represents computations we would like to perform every day. A freshly-constructed pipeline, like the one we just created, is empty. This means it doesn’t yet know how to compute anything, and it won’t produce any values if we ask for its outputs. In the sections below, we will see how to provide our Pipeline with expressions to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factors and Filters\n",
    "\n",
    "The `.AverageDollarVolume()` class used above is an example of a factor. In this section we will take a look at two types of computations that can be expressed in a pipeline: **Factors** and **Filters**. In general, factors and filters represent functions that produce a value from an asset in a moment in time, but are distinguished by the types of values they produce. Let's start by looking at factors.\n",
    "\n",
    "\n",
    "### Factors\n",
    "\n",
    "In general, a **Factor** is a function from an asset at a particular moment of time to a numerical value. A simple example of a factor is the most recent price of a security. Given a security and a specific moment in time, the most recent price is a number. Another example is the 10-day average trading volume of a security. Factors are most commonly used to assign values to securities which can then be combined with filters or other factors. The fact that you can combine multiple factors makes it easy for you to form new custom factors that can be as complex as you like. For example, constructing a Factor that computes the average of two other Factors can be simply illustrated usingthe pseudocode below:\n",
    "\n",
    "```python\n",
    "f1 = factor1(...)\n",
    "f2 = factor2(...)  \n",
    "average = (f1 + f2) / 2.0  \n",
    "```\n",
    "\n",
    "### Filters\n",
    "\n",
    "In general, a **Filter** is a function from an asset at a particular moment in time to a boolean value (True of False). An example of a filter is a function indicating whether a security's price is below \\$5. Given a security and a specific moment in time, this evaluates to either **True** or **False**. Filters are most commonly used for selecting sets of securities to include or exclude from your stock universe. Filters are usually applied using comparison operators, such as <, <=, !=, ==, >, >=."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing the Pipeline as a Diagram\n",
    "\n",
    "Zipline's Pipeline class comes with the attribute `.show_graph()` that allows you to render the Pipeline as a Directed Acyclic Graph (DAG). This graph is specified using the DOT language and consequently we need a DOT graph layout program to view the rendered image. In the code below, we will use the Graphviz pakage to render the graph produced by the `.show_graph()` attribute. Graphviz is an open-source package for drawing graphs specified in DOT language scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "# Render the pipeline as a DAG\n",
    "pipeline.show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, our pipeline is empty and it only contains a screen. Therefore, when we rendered our `pipeline`, we only see the diagram of our `screen`:\n",
    "\n",
    "```python\n",
    "AverageDollarVolume(window_length = 60).top(10)\n",
    "```\n",
    "\n",
    "By default, the `.AverageDollarVolume()` class uses the `USEquityPricing` dataset, containing daily trading prices and volumes, to compute the average dollar volume:\n",
    "\n",
    "```python\n",
    "average_dollar_volume = np.nansum(close_price * volume, axis=0) / len(close_price)\n",
    "```\n",
    "The top of the diagram reflects the fact that the `.AverageDollarVolume()` class gets its inputs (closing price and volume) from the `USEquityPricing` dataset. The bottom of the diagram shows that the output is determined by the expression `x_0 <= 10`. This expression reflects the fact that we used `.top(10)` as a filter in our `screen`. We refer to each box in the diagram as a Term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and Dataloaders               \n",
    "\n",
    "One of the features of Zipline's Pipeline is that it separates the actual source of the stock data from the abstract description of that dataset. Therefore, Zipline employs **DataSets** and **Loaders** for those datasets. `DataSets` are just abstract collections of sentinel values describing the columns/types for a particular dataset.  While a `loader` is an object which, given a request for a particular chunk of a dataset, can actually get the requested data. For example, the loader used for the `USEquityPricing` dataset, is the `USEquityPricingLoader` class. The `USEquityPricingLoader` class will delegate the loading of baselines and adjustments to lower-level subsystems that know how to get the pricing data in the default formats used by Zipline (`bcolz` for pricing data, and `SQLite` for split/merger/dividend data). As we saw in the beginning of this notebook, data bundles automatically convert the stock data into `bcolz` and `SQLite` formats. It is important to note that the `USEquityPricingLoader` class can also be used to load daily OHLCV data from other datasets, not just from the `USEquityPricing` dataset. Simliarly, it is also  possible to write different loaders for the same dataset and use those instead of the default loader. Zipline contains lots of other loaders to allow you to load data from different datasets.\n",
    "\n",
    "In the code below, we will use `USEquityPricingLoader(BcolzDailyBarWriter, SQLiteAdjustmentWriter)` to create a loader from a `bcolz` equity pricing directory and a `SQLite` adjustments path. Both the `BcolzDailyBarWriter` and `SQLiteAdjustmentWriter` determine the path of the pricing and adjustment data. Since we will be using the Quotemedia data bundle, we will use the `bundle_data.equity_daily_bar_reader` and the `bundle_data.adjustment_reader` as our `BcolzDailyBarWriter` and `SQLiteAdjustmentWriter`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline.loaders import USEquityPricingLoader\n",
    "\n",
    "# Set the dataloader\n",
    "pricing_loader = USEquityPricingLoader(bundle_data.equity_daily_bar_reader, bundle_data.adjustment_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Engine\n",
    "\n",
    "Zipline employs computation engines for executing Pipelines. In the code below we will use Zipline's `SimplePipelineEngine()` class as the engine to execute our pipeline. The `SimplePipelineEngine(get_loader, calendar, asset_finder)` class associates the chosen data loader with the corresponding dataset and a trading calendar. The `get_loader` parameter must be a callable function that is given a loadable term and returns a `PipelineLoader` to use to retrieve the raw data for that term in the pipeline. In our case, we will be using the `pricing_loader` defined above, we therefore, create a function called `choose_loader` that returns our `pricing_loader`. The function also checks that the data that is being requested corresponds to OHLCV data, otherwise it retunrs an error. The `calendar` parameter must be a `DatetimeIndex` array of dates to consider as trading days when computing a range between a fixed `start_date` and `end_date`. In our case, we will be using the same trading days as those used in the NYSE. We will use Zipline's `get_calendar('NYSE')` function to retrieve the trading days used by the NYSE. We then use the `.all_sessions` attribute to get the `DatetimeIndex` from our `trading_calendar` and pass it to the `calendar` parameter. Finally, the `asset_finder` parameter determines which assets are in the top-level universe of our stock data at any point in time. Since we are using the Quotemedia data bundle, we set this parameter to the `bundle_data.asset_finder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.utils.calendars import get_calendar\n",
    "from zipline.pipeline.data import USEquityPricing\n",
    "from zipline.pipeline.engine import SimplePipelineEngine\n",
    "\n",
    "# Define the function for the get_loader parameter\n",
    "def choose_loader(column):\n",
    "    if column not in USEquityPricing.columns:\n",
    "        raise Exception('Column not in USEquityPricing')\n",
    "    return pricing_loader\n",
    "\n",
    "# Set the trading calendar\n",
    "trading_calendar = get_calendar('NYSE')\n",
    "\n",
    "# Create a Pipeline engine\n",
    "engine = SimplePipelineEngine(get_loader = choose_loader,\n",
    "                              calendar = trading_calendar.all_sessions,\n",
    "                              asset_finder = bundle_data.asset_finder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a Pipeline\n",
    "\n",
    "Once we have chosen our engine we are ready to run or execute our pipeline. We can run our pipeline by using the `.run_pipeline()` attribute of the `SimplePipelineEngine` class. In particular, the `SimplePipelineEngine.run_pipeline(pipeline, start_date, end_date)` implements the following algorithm for executing pipelines:\n",
    "\n",
    "\n",
    "1. Build a dependency graph of all terms in the `pipeline`. In this step, the graph is sorted topologically to determine the order in which we can compute the terms.\n",
    "\n",
    "\n",
    "2. Ask our AssetFinder for a “lifetimes matrix”, which should contain, for each date between `start_date` and `end_date`, a boolean value for each known asset indicating whether the asset existed on that date.\n",
    "\n",
    "\n",
    "3. Compute each term in the dependency order determined in step 1, caching the results in a a dictionary so that they can be fed into future terms.\n",
    "\n",
    "\n",
    "4. For each date, determine the number of assets passing the `pipeline` screen. The sum, $N$, of all these values is the total number of rows in our output Pandas Dataframe, so we pre-allocate an output array of length $N$ for each factor in terms.\n",
    "\n",
    "\n",
    "5. Fill in the arrays allocated in step 4 by copying computed values from our output cache into the corresponding rows.\n",
    "\n",
    "\n",
    "6. Stick the values computed in step 5 into a Pandas DataFrame and return it.\n",
    "\n",
    "In the code below, we run our pipeline for a single day, so our `start_date` and `end_date` will be the same. We then print some information about our `pipeline_output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the start and end dates\n",
    "start_date = pd.Timestamp('2016-01-05', tz = 'utc')\n",
    "end_date = pd.Timestamp('2016-01-05', tz = 'utc')\n",
    "\n",
    "# Run our pipeline for the given start and end dates\n",
    "pipeline_output = engine.run_pipeline(pipeline, start_date, end_date)\n",
    "\n",
    "# We print information about the pipeline output\n",
    "print('The pipeline output has type:', type(pipeline_output), '\\n')\n",
    "\n",
    "# We print whether the pipeline output is a MultiIndex Dataframe\n",
    "print('Is the pipeline output a MultiIndex Dataframe:', isinstance(pipeline_output.index, pd.core.index.MultiIndex), '\\n')\n",
    "\n",
    "# If the pipeline output is a MultiIndex Dataframe we print the two levels of the index\n",
    "if isinstance(pipeline_output.index, pd.core.index.MultiIndex):\n",
    "\n",
    "    # We print the index level 0\n",
    "    print('Index Level 0:\\n\\n', pipeline_output.index.get_level_values(0), '\\n')\n",
    "\n",
    "    # We print the index level 1\n",
    "    print('Index Level 1:\\n\\n', pipeline_output.index.get_level_values(1), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the return value of `.run_pipeline()` is a `MultiIndex` Pandas DataFrame containing a row for each asset that passed our pipeline’s screen. We can also see that the 0th level of the index contains the date and the 1st level of the index contains the tickers. In general, the returned Pandas DataFrame will also contain a column for each factor and filter we add to the pipeline using  `Pipeline.add()`. At this point we haven't added any factors or filters to our pipeline, consequently, the Pandas Dataframe will have no columns. In the following sections we will see how to add factors and filters to our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tickers\n",
    "\n",
    "We saw in the previous section, that the tickers of the stocks that passed our pipeline’s screen are contained in the 1st level of the index. Therefore, we can use the Pandas `.get_level_values(1).values.tolist()` method to get the tickers of those stocks and save them to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values in index level 1 and save them to a list\n",
    "universe_tickers = pipeline_output.index.get_level_values(1).values.tolist()\n",
    "\n",
    "# Display the tickers\n",
    "universe_tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data\n",
    "\n",
    "Now that we have the tickers for the stocks that passed our pipeline’s screen, we can get the historical stock data for those tickers from our data bundle. In order to get the historical data we need to use Zipline's `DataPortal` class. A `DataPortal` is an interface to all of the data that a Zipline simulation needs. In the code below, we will create a `DataPortal` and `get_pricing` function to get historical stock prices for our tickers. \n",
    "\n",
    "We have already seen most of the parameters used below when we create the `DataPortal`, so we won't explain them again here. The only new parameter is `first_trading_day`. The `first_trading_day` parameter is a `pd.Timestamp` indicating the first trading day for the simulation. We will set the first trading day to the first trading day in the data bundle. For more information on the `DataPortal` class see the [Zipline documentation](https://www.zipline.io/appendix.html?highlight=dataportal#zipline.data.data_portal.DataPortal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.data.data_portal import DataPortal\n",
    "\n",
    "# Create a data portal\n",
    "data_portal = DataPortal(bundle_data.asset_finder,\n",
    "                         trading_calendar = trading_calendar,\n",
    "                         first_trading_day = bundle_data.equity_daily_bar_reader.first_trading_day,\n",
    "                         equity_daily_reader = bundle_data.equity_daily_bar_reader,\n",
    "                         adjustment_reader = bundle_data.adjustment_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created a `data_portal` we will create a helper function, `get_pricing`, that gets the historical data from the `data_portal` for a given set of `start_date` and `end_date`. The `get_pricing` function takes various parameters: \n",
    "\n",
    "```python\n",
    "def get_pricing(data_portal, trading_calendar, assets, start_date, end_date, field='close')\n",
    "```\n",
    "\n",
    "\n",
    "The first two parameters, `data_portal` and `trading_calendar`, have already been defined above. The third paramter, `assets`, is a list of tickers. In our case we will use the tickers from the output of our pipeline, namely, `universe_tickers`. The fourth and fifth parameters are strings specifying the `start_date` and `end_date`. The function converts these two strings into Timestamps with a Custom Business Day frequency. The last parameter, `field`, is a string used to indicate which field to return. In our case we want to get the closing price, so we set `field='close`. \n",
    "\n",
    "The function returns the historical stock price data using the `.get_history_window()` attribute of the `DataPortal` class. This attribute returns a Pandas Dataframe containing the requested history window with the data fully adjusted. The `bar_count` parameter is an integer indicating the number of days to return. The number of days determines the number of rows of the returned dataframe. Both the `frequency` and `data_frequency` parameters are strings that indicate the frequency of the data to query, *i.e.* whether the data is in `daily` or `minute` intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pricing(data_portal, trading_calendar, assets, start_date, end_date, field='close'):\n",
    "    \n",
    "    # Set the given start and end dates to Timestamps. The frequency string C is used to\n",
    "    # indicate that a CustomBusinessDay DateOffset is used\n",
    "    end_dt = pd.Timestamp(end_date, tz='UTC', freq='C')\n",
    "    start_dt = pd.Timestamp(start_date, tz='UTC', freq='C')\n",
    "\n",
    "    # Get the locations of the start and end dates\n",
    "    end_loc = trading_calendar.closes.index.get_loc(end_dt)\n",
    "    start_loc = trading_calendar.closes.index.get_loc(start_dt)\n",
    "\n",
    "    # return the historical data for the given window\n",
    "    return data_portal.get_history_window(assets=assets, end_dt=end_dt, bar_count=end_loc - start_loc,\n",
    "                                          frequency='1d',\n",
    "                                          field=field,\n",
    "                                          data_frequency='daily')\n",
    "\n",
    "# Get the historical data for the given window\n",
    "historical_data = get_pricing(data_portal, trading_calendar, universe_tickers,\n",
    "                              start_date='2011-01-05', end_date='2016-01-05')\n",
    "# Display the historical data\n",
    "historical_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Alignment\n",
    "\n",
    "When pipeline returns with a date of, e.g., `2016-01-07` this includes data that would be known as of before the **market open** on `2016-01-07`. As such, if you ask for latest known values on each day, it will return the closing price from the day before and label the date `2016-01-07`. All factor values assume to be run prior to the open on the labeled day with data known before that point in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Factors and Filters\n",
    "\n",
    "Now that you know how build a pipeline and execute it, in this section we will see how we can add factors and filters to our pipeline. These factors and filters will determine the computations we want our pipeline to compute each day.\n",
    "\n",
    "We can add both factors and filters to our pipeline using the `.add(column, name)` method of the `Pipeline` class. The `column` parameter represetns the factor or filter to add to the pipeline. The `name` parameter is a string that determines the name of the column in the output Pandas Dataframe for that factor of fitler. As mentioned earlier, each factor and filter will appear as a column in the output dataframe of our pipeline. Let's start by adding a factor to our pipeline.\n",
    "\n",
    "### Factors\n",
    "\n",
    "In the code below, we will use Zipline's built-in `SimpleMovingAverage` factor to create a factor that computes the 15-day mean closing price of securities. We will then add this factor to our pipeline and use `.show_graph()` to see a diagram of our pipeline with the factor added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline.factors import SimpleMovingAverage\n",
    "\n",
    "# Create a factor that computes the 15-day mean closing price of securities\n",
    "mean_close_15 = SimpleMovingAverage(inputs = [USEquityPricing.close], window_length = 15)\n",
    "\n",
    "# Add the factor to our pipeline\n",
    "pipeline.add(mean_close_15, '15 Day MCP')\n",
    "\n",
    "# Render the pipeline as a DAG\n",
    "pipeline.show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the diagram above we can clearly see the factor we have added. Now, we can run our pipeline again and see its output. The pipeline is run in exactly the same way we did before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set starting and end dates\n",
    "start_date = pd.Timestamp('2014-01-06', tz='utc')\n",
    "end_date = pd.Timestamp('2016-01-05', tz='utc')\n",
    "\n",
    "# Run our pipeline for the given start and end dates\n",
    "output = engine.run_pipeline(pipeline, start_date, end_date)\n",
    "\n",
    "# Display the pipeline output\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now our output dataframe contains a column with the name `15 Day MCP`, which is the name we gave to our factor before. This ouput dataframe from our pipeline gives us the 15-day mean closing price of the securities that passed our `screen`.\n",
    "\n",
    "### Filters\n",
    "\n",
    "Filters are created and added to the pipeline in the same way as factors. In the code below, we create a filter that returns `True` whenever the 15-day average closing price is above \\$100. Remember, a filter produces a `True` or `False` value for each security every day. We will then add this filter to our pipeline and use `.show_graph()` to see a diagram of our pipeline with the filter added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Filter that returns True whenever the 15-day average closing price is above $100\n",
    "high_mean = mean_close_15 > 100\n",
    "\n",
    "# Add the filter to our pipeline\n",
    "pipeline.add(high_mean, 'High Mean')\n",
    "\n",
    "# Render the pipeline as a DAG\n",
    "pipeline.show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the diagram above we can clearly see the fiter we have added. Now, we can run our pipeline again and see its output. The pipeline is run in exactly the same way we did before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set starting and end dates\n",
    "start_date = pd.Timestamp('2014-01-06', tz='utc')\n",
    "end_date = pd.Timestamp('2016-01-05', tz='utc')\n",
    "\n",
    "# Run our pipeline for the given start and end dates\n",
    "output = engine.run_pipeline(pipeline, start_date, end_date)\n",
    "\n",
    "# Display the pipeline output\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now our output dataframe contains a two columns, one for the filter and one for the factor. The new column has the name `High Mean`, which is the name we gave to our filter before. Notice that the filter column only contains Boolean values, where only the securities with a 15-day average closing price above \\$100 have `True` values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_ldconf",
   "language": "python",
   "name": "myenv_ldconf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
