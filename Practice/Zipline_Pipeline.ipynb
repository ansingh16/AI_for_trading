{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipline Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "On any given trading day, the entire universe of stocks consists of thousands of securities. Usually, you will not be  interested in investing in all the stocks in the entire universe, but rather, you will likely select only a subset of these to invest. For example, you may only want to invest in stocks that have a 10-day average closing price of \\$10.00 or less. Or you may only want to invest in the top 500 securities ranked by some factor.\n",
    "\n",
    "In order to avoid spending a lot of time doing data wrangling to select only the securities you are interested in, people often use **pipelines**. In general, a pipeline is a placeholder for a series of data operations used to filter and rank data according to some factor or factors. \n",
    "\n",
    "In this notebook, you will learn how to work with the **Zipline Pipeline**. Zipline is an open-source algorithmic trading simulator developed by *Quantopian*. We will learn how to use the Zipline Pipeline to filter stock data according to factors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data with Zipline\n",
    "\n",
    "Before we build our pipeline with Zipline, we will first see how we can load the stock data we are going to use into Zipline. Zipline uses **Data Bundles** to make it easy to use different data sources. A data bundle is a collection of pricing data, adjustment data, and an asset database. Zipline employs data bundles to preload data used to run backtests and store data for future runs. Zipline comes with a few data bundles by default but it also has the ability to ingest new bundles. The first step to using a data bundle is to ingest the data. Zipline's ingestion process will start by downloading the data or by loading data files from your local machine. It will then pass the data to a set of writer objects that converts the original data to Zipline’s internal format (`bcolz` for pricing data, and `SQLite` for split/merger/dividend data) that hs been optimized for speed. This new data is written to a standard location that Zipline can find. By default, the new data is written to a subdirectory of `ZIPLINE_ROOT/data/<bundle>`, where `<bundle>` is the name given to the bundle ingested and the subdirectory is named with the current date. This allows Zipline to look at older data and run backtests on older copies of the data. Running a backtest with an old ingestion makes it easier to reproduce backtest results later. \n",
    "\n",
    "In this notebook, we will be using stock data from **Quotemedia**. In the Udacity Workspace you will find that the stock data from Quotemedia has already been ingested into Zipline. Therefore, in the code below we will use Zipline's `bundles.load()` function to load our previously ingested stock data from Quotemedia. In order to use the `bundles.load()` function we first need to do a couple of things. First, we need to specify the name of the bundle previously ingested. In this case, the name of the Quotemedia data bundle is `eod-quotemedia`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/495 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import tqdm\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Set up logging to capture errors\n",
    "logging.basicConfig(filename='download_errors.log', level=logging.ERROR)\n",
    "\n",
    "# Load CSV data\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/rbpal/01-qt-project-03-smart-beta-portfolio-optimization/main/eod-quotemedia.csv', index_col=False)\n",
    "\n",
    "# Convert 'date' column to datetime if it's not already\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Define output directory\n",
    "output_dir = '../Data/data/eod-quotemedia/daily'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Group by ticker and process each group\n",
    "for ticker, group in tqdm.tqdm(df.groupby('ticker')):\n",
    "    # Ensure 'date' is sorted and in datetime format\n",
    "    group = group.sort_values('date')\n",
    "    \n",
    "    # Get the start and end dates for this ticker\n",
    "    start_session = pd.Timestamp('2013-07-01', tz='UTC')\n",
    "    end_session = pd.Timestamp('2017-06-30', tz='UTC')\n",
    "    \n",
    "    try:\n",
    "        # Download data from yfinance\n",
    "        dat = yf.download(ticker, start=start_session, end=end_session, progress=False)\n",
    "        \n",
    "        # Check if data is returned\n",
    "        if dat.empty:\n",
    "            print(f\"No data returned for {ticker}.\")\n",
    "            continue\n",
    "\n",
    "        # Rename columns to match expected format\n",
    "        dat.columns = ['open', 'high', 'low', 'close', 'adj close', 'volume']\n",
    "        \n",
    "        # Rename index and ensure it's in datetime format\n",
    "        dat.index.name = 'date'\n",
    "        dat.index = dat.index.tz_localize(None)  # Remove timezone if needed\n",
    "        \n",
    "        # Write data to a CSV file with the ticker name\n",
    "        dat.to_csv(f'{output_dir}/{ticker}.csv', index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log and print errors\n",
    "        print(f\"Error processing {ticker}: {e}\")\n",
    "        logging.error(f\"Error processing {ticker}: {e}\")\n",
    "\n",
    "    # Break statement is for debugging; remove in production\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we need to register the data bundle and its ingest function with Zipline, using the `bundles.register()` function. The ingest function is responsible for loading the data into memory and passing it to a set of writer objects provided by Zipline to convert the data to Zipline’s internal format. Since the original Quotemedia data was contained in `.csv` files, we will use the `csvdir_equities()` function to generate the ingest function for our Quotemedia data bundle. In addition, since Quotemedia's `.csv` files contained daily stock data, we will set the time frame for our ingest function, to `daily`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bundle registration and ingestion completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from zipline.data.bundles import register, ingest\n",
    "from zipline.data.bundles.csvdir import csvdir_equities\n",
    "import logging\n",
    "\n",
    "# Set up logging to capture errors\n",
    "logging.basicConfig(filename='bundle_errors.log', level=logging.ERROR)\n",
    "\n",
    "# Define the bundle name and session dates\n",
    "# Ensure these timestamps are timezone-naive or all timezone-aware (consistent with your data)\n",
    "start_session = pd.Timestamp('2013-07-01')  # Timezone-naive\n",
    "end_session = pd.Timestamp('2017-06-30')    # Timezone-naive\n",
    "\n",
    "# If your data is timezone-aware, you should use the following:\n",
    "# start_session = pd.Timestamp('2013-07-01', tz='UTC')  # Timezone-aware\n",
    "# end_session = pd.Timestamp('2017-06-30', tz='UTC')    # Timezone-aware\n",
    "\n",
    "try:\n",
    "    # Register the bundle\n",
    "    register(\n",
    "        'eod-quotemedia',\n",
    "        csvdir_equities(\n",
    "            ['daily'],  # This specifies the frequency directory\n",
    "            '../Data/data/eod-quotemedia/'  # Path to the directory containing data\n",
    "        ),\n",
    "        calendar_name='NYSE',  # US equities\n",
    "        start_session=start_session,\n",
    "        end_session=end_session\n",
    "    )\n",
    "\n",
    "    # Ingest the bundle\n",
    "    # ingest('eod-quotemedia')\n",
    "\n",
    "    print(\"Bundle registration and ingestion completed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Log and print errors\n",
    "    print(f\"Error during bundle registration or ingestion: {e}\")\n",
    "    logging.error(f\"Error during bundle registration or ingestion: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our data bundle and ingest function are registered, we can load our data using the `bundles.load()` function. Since this function loads our previously ingested data, we need to set `ZIPLINE_ROOT` to the path of the most recent ingested data. The most recent data is located in the `../Data/data/eod-quotemedia/` directory, where `cwd` is the current working directory. We will specify this location using the `os.environ[]` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipline\n",
    "from zipline.api import order, record, symbol\n",
    "from zipline.algorithm import TradingAlgorithm\n",
    "from zipline.data import bundles\n",
    "from zipline.utils.calendar_utils import get_calendar\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the custom bundle\n",
    "bundle_name = 'eod-quotemedia'\n",
    "bundle_data = bundles.load(bundle_name)\n",
    "\n",
    "# Define the trading calendar (e.g., NYSE)\n",
    "trading_calendar = get_calendar('NYSE')\n",
    "\n",
    "# Define a simple algorithm\n",
    "def initialize(context):\n",
    "    context.asset = symbol('A')\n",
    "\n",
    "def handle_data(context, data):\n",
    "    order(context.asset, 10)\n",
    "    record(A=data.current(context.asset, 'price'))\n",
    "\n",
    "# Define the start and end date for the backtest\n",
    "start_date = pd.Timestamp('2013-07-01', tz='utc')\n",
    "end_date = pd.Timestamp('2017-06-30', tz='utc')\n",
    "\n",
    "# Create the algorithm object\n",
    "algo = TradingAlgorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asset ID: 0\n",
      "Symbol: A\n",
      "Start Date: 2013-07-01 00:00:00\n",
      "End Date: 2017-06-29 00:00:00\n",
      "Exchange: CSVDIR\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "import zipline\n",
    "from zipline.data import bundles\n",
    "\n",
    "# Load the custom bundle\n",
    "bundle_name = 'eod-quotemedia'\n",
    "bundle_data = bundles.load(bundle_name)\n",
    "\n",
    "# Access the AssetFinder\n",
    "asset_finder = bundle_data.asset_finder\n",
    "\n",
    "# Get all assets in the bundle\n",
    "assets = asset_finder.retrieve_all(asset_finder.sids)\n",
    "\n",
    "# Print metadata for all assets\n",
    "for asset in assets:\n",
    "    print(f\"Asset ID: {asset.sid}\")\n",
    "    print(f\"Symbol: {asset.symbol}\")\n",
    "    print(f\"Start Date: {asset.start_date}\")\n",
    "    print(f\"End Date: {asset.end_date}\")\n",
    "    print(f\"Exchange: {asset.exchange}\")\n",
    "    print(\"---------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exchanges\n",
      "asset_router\n",
      "version_info\n",
      "equities\n",
      "futures_root_symbols\n",
      "equity_symbol_mappings\n",
      "equity_supplementary_mappings\n",
      "futures_contracts\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "# Path to your bundle's assets database\n",
    "bundle_db_path = '../Data/data/eod-quotemedia/2024-09-04T09;37;22.939161/assets-7.sqlite'\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(bundle_db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute a query to list all tables\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "\n",
    "# Fetch all results from the executed query\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "# Print the list of tables\n",
    "for table in tables:\n",
    "    print(table[0])\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table: exchanges\n",
      "('XNYS', 'CSVDIR', 'US')\n",
      "\n",
      "Table: asset_router\n",
      "(0, 'equity')\n",
      "\n",
      "Table: version_info\n",
      "(1, 7)\n",
      "\n",
      "Table: equities\n",
      "(0, None, 1372636800000000000, 1498694400000000000, -9223372036854775808, 1498780800000000000, 'CSVDIR')\n",
      "\n",
      "Table: futures_root_symbols\n",
      "\n",
      "Table: equity_symbol_mappings\n",
      "(0, 0, 'A', 'A', '', 1372636800000000000, 1498694400000000000)\n",
      "\n",
      "Table: equity_supplementary_mappings\n",
      "\n",
      "Table: futures_contracts\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Path to your SQLite database file\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(bundle_db_path)\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Define the list of tables you're interested in\n",
    "tables_of_interest = [\n",
    "    'exchanges', 'asset_router', 'version_info', 'equities',\n",
    "    'futures_root_symbols', 'equity_symbol_mappings', 'equity_supplementary_mappings', 'futures_contracts'\n",
    "]\n",
    "\n",
    "# Iterate over each table and display its contents\n",
    "for table in tables_of_interest:\n",
    "    print(f\"\\nTable: {table}\")\n",
    "    cursor.execute(f\"SELECT * FROM {table} LIMIT 5;\")  # LIMIT 5 to only show the first 5 rows\n",
    "    rows = cursor.fetchall()\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated rows in 'exchanges':\n",
      "('XNYS', 'CSVDIR', 'US')\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Path to your SQLite database file\n",
    "# db_path = 'path_to_your_database_file/assets-7.sqlite'\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(bundle_db_path)\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Update the exchange name from 'CSVDIR' to 'XNYS' and set the country code to 'US'\n",
    "cursor.execute(\"\"\"\n",
    "    UPDATE exchanges\n",
    "    SET exchange = 'XNYS', country_code = 'US'\n",
    "    WHERE exchange = 'CSVDIR';\n",
    "\"\"\")\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()\n",
    "\n",
    "# Verify the update by fetching the modified rows\n",
    "cursor.execute(\"SELECT * FROM exchanges WHERE exchange = 'XNYS';\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "print(\"Updated rows in 'exchanges':\")\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Empty Pipeline\n",
    "\n",
    "Once we have loaded our data, we can start building our Zipline pipeline. We begin by creating an empty Pipeline object using Zipline's `Pipeline` class. A Pipeline object represents a collection of named expressions to be compiled and executed by a Pipeline Engine. The `Pipeline(columns=None, screen=None)` class takes two optional parameters, `columns` and `screen`. The `columns` parameter is a dictionary used to indicate the intial columns to use, and the `screen` parameter is used to setup a screen to exclude unwanted data. \n",
    "\n",
    "In the code below we will create a `screen` for our pipeline using Zipline's built-in `.AverageDollarVolume()` class. We will use the `.AverageDollarVolume()` class to produce a 60-day Average Dollar Volume of closing prices for every stock in our universe. We then use the `.top(10)` attribute to specify that we want to filter down our universe each day to just the top 10 assets. Therefore, this screen will act as a filter to exclude data from our stock universe each day. The average dollar volume is a good first pass filter to avoid illiquid assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Pipeline.__init__() expected a value of type zipline.pipeline.domain.Domain for argument 'domain', but got NoneType instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m universe \u001b[38;5;241m=\u001b[39m AverageDollarVolume(window_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60\u001b[39m)\u001b[38;5;241m.\u001b[39mtop(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create an empty Pipeline with the given screen\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscreen\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muniverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Finance/lib/python3.9/site-packages/zipline/pipeline/pipeline.py:5\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, columns, screen, domain)\u001b[0m\n\u001b[1;32m      1\u001b[0m from zipline.errors import UnsupportedPipelineOutput\n\u001b[1;32m      2\u001b[0m from zipline.utils.input_validation import (\n\u001b[1;32m      3\u001b[0m     expect_element,\n\u001b[1;32m      4\u001b[0m     expect_types,\n\u001b[0;32m----> 5\u001b[0m     optional,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m from .domain import Domain, GENERIC, infer_domain\n\u001b[1;32m      9\u001b[0m from .graph import ExecutionPlan, TermGraph, SCREEN_NAME\n",
      "File \u001b[0;32m~/miniconda3/envs/Finance/lib/python3.9/site-packages/zipline/utils/input_validation.py:442\u001b[0m, in \u001b[0;36mmake_check.<locals>._check\u001b[0;34m(func, argname, argvalue)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check\u001b[39m(func, argname, argvalue):\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pred(argvalue):\n\u001b[0;32m--> 442\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc_type(\n\u001b[1;32m    443\u001b[0m             template\n\u001b[1;32m    444\u001b[0m             \u001b[38;5;241m%\u001b[39m {\n\u001b[1;32m    445\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuncname\u001b[39m\u001b[38;5;124m\"\u001b[39m: get_funcname(func),\n\u001b[1;32m    446\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margname\u001b[39m\u001b[38;5;124m\"\u001b[39m: argname,\n\u001b[1;32m    447\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual\u001b[39m\u001b[38;5;124m\"\u001b[39m: actual(argvalue),\n\u001b[1;32m    448\u001b[0m             },\n\u001b[1;32m    449\u001b[0m         )\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m argvalue\n",
      "\u001b[0;31mTypeError\u001b[0m: Pipeline.__init__() expected a value of type zipline.pipeline.domain.Domain for argument 'domain', but got NoneType instead."
     ]
    }
   ],
   "source": [
    "from zipline.pipeline import Pipeline\n",
    "from zipline.pipeline.factors import AverageDollarVolume\n",
    "from zipline.pipeline.domain import US_EQUITIES\n",
    "\n",
    "# Create a screen for our Pipeline\n",
    "universe = AverageDollarVolume(window_length = 60).top(10)\n",
    "\n",
    "# Create an empty Pipeline with the given screen\n",
    "pipeline = Pipeline(screen = universe,domain=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGENERIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "A Pipeline object represents a collection of named expressions to be\n",
      "compiled and executed by a PipelineEngine.\n",
      "\n",
      "A Pipeline has two important attributes: 'columns', a dictionary of named\n",
      ":class:`~zipline.pipeline.Term` instances, and 'screen', a\n",
      ":class:`~zipline.pipeline.Filter` representing criteria for\n",
      "including an asset in the results of a Pipeline.\n",
      "\n",
      "To compute a pipeline in the context of a TradingAlgorithm, users must call\n",
      "``attach_pipeline`` in their ``initialize`` function to register that the\n",
      "pipeline should be computed each trading day. The most recent outputs of an\n",
      "attached pipeline can be retrieved by calling ``pipeline_output`` from\n",
      "``handle_data``, ``before_trading_start``, or a scheduled function.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "columns : dict, optional\n",
      "    Initial columns.\n",
      "screen : zipline.pipeline.Filter, optional\n",
      "    Initial screen.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/Finance/lib/python3.9/site-packages/zipline/pipeline/pipeline.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "Pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above we have named our Pipeline object `pipeline` so that we can identify it later when we make computations. Remember a Pipeline is an object that represents computations we would like to perform every day. A freshly-constructed pipeline, like the one we just created, is empty. This means it doesn’t yet know how to compute anything, and it won’t produce any values if we ask for its outputs. In the sections below, we will see how to provide our Pipeline with expressions to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factors and Filters\n",
    "\n",
    "The `.AverageDollarVolume()` class used above is an example of a factor. In this section we will take a look at two types of computations that can be expressed in a pipeline: **Factors** and **Filters**. In general, factors and filters represent functions that produce a value from an asset in a moment in time, but are distinguished by the types of values they produce. Let's start by looking at factors.\n",
    "\n",
    "\n",
    "### Factors\n",
    "\n",
    "In general, a **Factor** is a function from an asset at a particular moment of time to a numerical value. A simple example of a factor is the most recent price of a security. Given a security and a specific moment in time, the most recent price is a number. Another example is the 10-day average trading volume of a security. Factors are most commonly used to assign values to securities which can then be combined with filters or other factors. The fact that you can combine multiple factors makes it easy for you to form new custom factors that can be as complex as you like. For example, constructing a Factor that computes the average of two other Factors can be simply illustrated usingthe pseudocode below:\n",
    "\n",
    "```python\n",
    "f1 = factor1(...)\n",
    "f2 = factor2(...)  \n",
    "average = (f1 + f2) / 2.0  \n",
    "```\n",
    "\n",
    "### Filters\n",
    "\n",
    "In general, a **Filter** is a function from an asset at a particular moment in time to a boolean value (True of False). An example of a filter is a function indicating whether a security's price is below \\$5. Given a security and a specific moment in time, this evaluates to either **True** or **False**. Filters are most commonly used for selecting sets of securities to include or exclude from your stock universe. Filters are usually applied using comparison operators, such as <, <=, !=, ==, >, >=."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing the Pipeline as a Diagram\n",
    "\n",
    "Zipline's Pipeline class comes with the attribute `.show_graph()` that allows you to render the Pipeline as a Directed Acyclic Graph (DAG). This graph is specified using the DOT language and consequently we need a DOT graph layout program to view the rendered image. In the code below, we will use the Graphviz pakage to render the graph produced by the `.show_graph()` attribute. Graphviz is an open-source package for drawing graphs specified in DOT language scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"353pt\" height=\"376pt\" viewBox=\"0.00 0.00 353.00 376.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 372)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-372 349,-372 349,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_Output</title>\n",
       "<polygon fill=\"#ffec8b\" stroke=\"#ffec8b\" points=\"98,-8 98,-85 246,-85 246,-8 98,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"172\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\">Output</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_Input</title>\n",
       "<polygon fill=\"#ffec8b\" stroke=\"#ffec8b\" points=\"8,-268 8,-360 337,-360 337,-268 8,-268\"/>\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-344.8\" font-family=\"Times,serif\" font-size=\"14.00\">Input</text>\n",
       "</g>\n",
       "<!-- 139885464312512 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139885464312512</title>\n",
       "<polygon fill=\"#ccebc5\" stroke=\"black\" points=\"237.5,-77 106.5,-77 106.5,-39 237.5,-39 237.5,-77\"/>\n",
       "<text text-anchor=\"start\" x=\"114.5\" y=\"-61.8\" font-family=\"Times,serif\" font-size=\"14.00\">Expression:</text>\n",
       "<text text-anchor=\"start\" x=\"114.5\" y=\"-46.8\" font-family=\"Times,serif\" font-size=\"14.00\">  x_0 &lt;= (1.00E+01)</text>\n",
       "</g>\n",
       "<!-- 139885482141920 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139885482141920</title>\n",
       "<polygon fill=\"#fbb4ae\" stroke=\"black\" points=\"328.5,-329 181.5,-329 181.5,-276 328.5,-276 328.5,-329\"/>\n",
       "<text text-anchor=\"start\" x=\"189.5\" y=\"-313.8\" font-family=\"Times,serif\" font-size=\"14.00\">BoundColumn:</text>\n",
       "<text text-anchor=\"start\" x=\"189.5\" y=\"-298.8\" font-family=\"Times,serif\" font-size=\"14.00\">  Dataset: EquityPricing</text>\n",
       "<text text-anchor=\"start\" x=\"189.5\" y=\"-283.8\" font-family=\"Times,serif\" font-size=\"14.00\">  Column: volume</text>\n",
       "</g>\n",
       "<!-- 139885464310496 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139885464310496</title>\n",
       "<polygon fill=\"#b3cde3\" stroke=\"black\" points=\"243.5,-240 100.5,-240 100.5,-202 243.5,-202 243.5,-240\"/>\n",
       "<text text-anchor=\"start\" x=\"108.5\" y=\"-224.8\" font-family=\"Times,serif\" font-size=\"14.00\">AverageDollarVolume:</text>\n",
       "<text text-anchor=\"start\" x=\"108.5\" y=\"-209.8\" font-family=\"Times,serif\" font-size=\"14.00\">  window_length: 60</text>\n",
       "</g>\n",
       "<!-- 139885482141920&#45;&gt;139885464310496 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>139885482141920-&gt;139885464310496</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M212.5,-275.81C212.5,-275.81 212.5,-250.25 212.5,-250.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"216,-250.25 212.5,-240.25 209,-250.25 216,-250.25\"/>\n",
       "</g>\n",
       "<!-- 139885482227696 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139885482227696</title>\n",
       "<polygon fill=\"#fbb4ae\" stroke=\"black\" points=\"163.5,-329 16.5,-329 16.5,-276 163.5,-276 163.5,-329\"/>\n",
       "<text text-anchor=\"start\" x=\"24.5\" y=\"-313.8\" font-family=\"Times,serif\" font-size=\"14.00\">BoundColumn:</text>\n",
       "<text text-anchor=\"start\" x=\"24.5\" y=\"-298.8\" font-family=\"Times,serif\" font-size=\"14.00\">  Dataset: EquityPricing</text>\n",
       "<text text-anchor=\"start\" x=\"24.5\" y=\"-283.8\" font-family=\"Times,serif\" font-size=\"14.00\">  Column: close</text>\n",
       "</g>\n",
       "<!-- 139885482227696&#45;&gt;139885464310496 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139885482227696-&gt;139885464310496</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M132,-275.81C132,-275.81 132,-250.25 132,-250.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"135.5,-250.25 132,-240.25 128.5,-250.25 135.5,-250.25\"/>\n",
       "</g>\n",
       "<!-- 139885464312992 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>139885464312992</title>\n",
       "<polygon fill=\"#b3cde3\" stroke=\"black\" points=\"233.5,-166 110.5,-166 110.5,-113 233.5,-113 233.5,-166\"/>\n",
       "<text text-anchor=\"start\" x=\"118.5\" y=\"-150.8\" font-family=\"Times,serif\" font-size=\"14.00\">Rank:</text>\n",
       "<text text-anchor=\"start\" x=\"118.5\" y=\"-135.8\" font-family=\"Times,serif\" font-size=\"14.00\">  method: 'ordinal'</text>\n",
       "<text text-anchor=\"start\" x=\"118.5\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\">  mask: AssetExists</text>\n",
       "</g>\n",
       "<!-- 139885464310496&#45;&gt;139885464312992 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139885464310496-&gt;139885464312992</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M172,-201.69C172,-201.69 172,-176.08 172,-176.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"175.5,-176.08 172,-166.08 168.5,-176.08 175.5,-176.08\"/>\n",
       "</g>\n",
       "<!-- 139885464312992&#45;&gt;139885464312512 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139885464312992-&gt;139885464312512</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M172,-112.81C172,-112.81 172,-87.25 172,-87.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"175.5,-87.25 172,-77.25 168.5,-87.25 175.5,-87.25\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "\n",
    "# Render the pipeline as a DAG\n",
    "pipeline.show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, our pipeline is empty and it only contains a screen. Therefore, when we rendered our `pipeline`, we only see the diagram of our `screen`:\n",
    "\n",
    "```python\n",
    "AverageDollarVolume(window_length = 60).top(10)\n",
    "```\n",
    "\n",
    "By default, the `.AverageDollarVolume()` class uses the `USEquityPricing` dataset, containing daily trading prices and volumes, to compute the average dollar volume:\n",
    "\n",
    "```python\n",
    "average_dollar_volume = np.nansum(close_price * volume, axis=0) / len(close_price)\n",
    "```\n",
    "The top of the diagram reflects the fact that the `.AverageDollarVolume()` class gets its inputs (closing price and volume) from the `USEquityPricing` dataset. The bottom of the diagram shows that the output is determined by the expression `x_0 <= 10`. This expression reflects the fact that we used `.top(10)` as a filter in our `screen`. We refer to each box in the diagram as a Term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and Dataloaders               \n",
    "\n",
    "One of the features of Zipline's Pipeline is that it separates the actual source of the stock data from the abstract description of that dataset. Therefore, Zipline employs **DataSets** and **Loaders** for those datasets. `DataSets` are just abstract collections of sentinel values describing the columns/types for a particular dataset.  While a `loader` is an object which, given a request for a particular chunk of a dataset, can actually get the requested data. For example, the loader used for the `USEquityPricing` dataset, is the `USEquityPricingLoader` class. The `USEquityPricingLoader` class will delegate the loading of baselines and adjustments to lower-level subsystems that know how to get the pricing data in the default formats used by Zipline (`bcolz` for pricing data, and `SQLite` for split/merger/dividend data). As we saw in the beginning of this notebook, data bundles automatically convert the stock data into `bcolz` and `SQLite` formats. It is important to note that the `USEquityPricingLoader` class can also be used to load daily OHLCV data from other datasets, not just from the `USEquityPricing` dataset. Simliarly, it is also  possible to write different loaders for the same dataset and use those instead of the default loader. Zipline contains lots of other loaders to allow you to load data from different datasets.\n",
    "\n",
    "In the code below, we will use `USEquityPricingLoader(BcolzDailyBarWriter, SQLiteAdjustmentWriter)` to create a loader from a `bcolz` equity pricing directory and a `SQLite` adjustments path. Both the `BcolzDailyBarWriter` and `SQLiteAdjustmentWriter` determine the path of the pricing and adjustment data. Since we will be using the Quotemedia data bundle, we will use the `bundle_data.equity_daily_bar_reader` and the `bundle_data.adjustment_reader` as our `BcolzDailyBarWriter` and `SQLiteAdjustmentWriter`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline.loaders import USEquityPricingLoader\n",
    "\n",
    "# Set the dataloader\n",
    "pricing_loader = USEquityPricingLoader(bundle_data.equity_daily_bar_reader, bundle_data.adjustment_reader,fx_reader=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mUSEquityPricingLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_price_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjustments_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfx_reader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "A PipelineLoader for loading daily OHLCV data.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "raw_price_reader : zipline.data.session_bars.SessionBarReader\n",
      "    Reader providing raw prices.\n",
      "adjustments_reader : zipline.data.adjustments.SQLiteAdjustmentReader\n",
      "    Reader providing price/volume adjustments.\n",
      "fx_reader : zipline.data.fx.FXRateReader\n",
      "   Reader providing currency conversions.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/Finance/lib/python3.9/site-packages/zipline/pipeline/loaders/equity_pricing_loader.py\n",
      "\u001b[0;31mType:\u001b[0m           ImplementsMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "USEquityPricingLoader?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Engine\n",
    "\n",
    "Zipline employs computation engines for executing Pipelines. In the code below we will use Zipline's `SimplePipelineEngine()` class as the engine to execute our pipeline. The `SimplePipelineEngine(get_loader, calendar, asset_finder)` class associates the chosen data loader with the corresponding dataset and a trading calendar. The `get_loader` parameter must be a callable function that is given a loadable term and returns a `PipelineLoader` to use to retrieve the raw data for that term in the pipeline. In our case, we will be using the `pricing_loader` defined above, we therefore, create a function called `choose_loader` that returns our `pricing_loader`. The function also checks that the data that is being requested corresponds to OHLCV data, otherwise it retunrs an error. The `calendar` parameter must be a `DatetimeIndex` array of dates to consider as trading days when computing a range between a fixed `start_date` and `end_date`. In our case, we will be using the same trading days as those used in the NYSE. We will use Zipline's `get_calendar('NYSE')` function to retrieve the trading days used by the NYSE. We then use the `.all_sessions` attribute to get the `DatetimeIndex` from our `trading_calendar` and pass it to the `calendar` parameter. Finally, the `asset_finder` parameter determines which assets are in the top-level universe of our stock data at any point in time. Since we are using the Quotemedia data bundle, we set this parameter to the `bundle_data.asset_finder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipline.utils.calendar_utils as utils\n",
    "from zipline.pipeline.data import USEquityPricing\n",
    "from zipline.pipeline.engine import SimplePipelineEngine\n",
    "\n",
    "# Define the function for the get_loader parameter\n",
    "def choose_loader(column):\n",
    "    if column not in USEquityPricing.columns:\n",
    "        raise Exception('Column not in USEquityPricing')\n",
    "    return pricing_loader\n",
    "\n",
    "# Set the trading calendar\n",
    "trading_calendar = utils.get_calendar('NYSE')\n",
    "\n",
    "# Create a Pipeline engine\n",
    "engine = SimplePipelineEngine(get_loader = choose_loader,\n",
    "                              asset_finder = bundle_data.asset_finder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a Pipeline\n",
    "\n",
    "Once we have chosen our engine we are ready to run or execute our pipeline. We can run our pipeline by using the `.run_pipeline()` attribute of the `SimplePipelineEngine` class. In particular, the `SimplePipelineEngine.run_pipeline(pipeline, start_date, end_date)` implements the following algorithm for executing pipelines:\n",
    "\n",
    "\n",
    "1. Build a dependency graph of all terms in the `pipeline`. In this step, the graph is sorted topologically to determine the order in which we can compute the terms.\n",
    "\n",
    "\n",
    "2. Ask our AssetFinder for a “lifetimes matrix”, which should contain, for each date between `start_date` and `end_date`, a boolean value for each known asset indicating whether the asset existed on that date.\n",
    "\n",
    "\n",
    "3. Compute each term in the dependency order determined in step 1, caching the results in a a dictionary so that they can be fed into future terms.\n",
    "\n",
    "\n",
    "4. For each date, determine the number of assets passing the `pipeline` screen. The sum, $N$, of all these values is the total number of rows in our output Pandas Dataframe, so we pre-allocate an output array of length $N$ for each factor in terms.\n",
    "\n",
    "\n",
    "5. Fill in the arrays allocated in step 4 by copying computed values from our output cache into the corresponding rows.\n",
    "\n",
    "\n",
    "6. Stick the values computed in step 5 into a Pandas DataFrame and return it.\n",
    "\n",
    "In the code below, we run our pipeline for a single day, so our `start_date` and `end_date` will be the same. We then print some information about our `pipeline_output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First trading day: 2013-07-01 00:00:00\n",
      "Last trading day: 2017-06-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from zipline.data.bundles import  load\n",
    "\n",
    "bundle_data = load('eod-quotemedia')\n",
    "first_trading_day = bundle_data.equity_daily_bar_reader.first_trading_day\n",
    "last_trading_day = bundle_data.equity_daily_bar_reader.last_available_dt\n",
    "\n",
    "print(f\"First trading day: {first_trading_day}\")\n",
    "print(f\"Last trading day: {last_trading_day}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find any assets with country_code 'US' that traded between 2016-01-05 00:00:00 and 2016-01-06 00:00:00.\nThis probably means that your asset db is old or that it has incorrect country/exchange metadata.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 17\u001b[0m\n\u001b[1;32m      5\u001b[0m end_date \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimestamp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2016-01-06\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# check if start_date is in sessions\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# all_sessions = pipeline.domain(default=US_EQUITIES).sessions().to_list()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Run our pipeline for the given start and end dates\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m pipeline_output \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# We print information about the pipeline output\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe pipeline output has type:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m(pipeline_output), \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Finance/lib/python3.9/site-packages/zipline/pipeline/engine.py:363\u001b[0m, in \u001b[0;36mSimplePipelineEngine.run_pipeline\u001b[0;34m(self, pipeline, start_date, end_date, hooks)\u001b[0m\n\u001b[1;32m    361\u001b[0m hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_hooks(hooks)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hooks\u001b[38;5;241m.\u001b[39mrunning_pipeline(pipeline, start_date, end_date):\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_pipeline_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Finance/lib/python3.9/site-packages/zipline/pipeline/engine.py:389\u001b[0m, in \u001b[0;36mSimplePipelineEngine._run_pipeline_impl\u001b[0;34m(self, pipeline, start_date, end_date, hooks)\u001b[0m\n\u001b[1;32m    382\u001b[0m plan \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mto_execution_plan(\n\u001b[1;32m    383\u001b[0m     domain,\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root_mask_term,\n\u001b[1;32m    385\u001b[0m     start_date,\n\u001b[1;32m    386\u001b[0m     end_date,\n\u001b[1;32m    387\u001b[0m )\n\u001b[1;32m    388\u001b[0m extra_rows \u001b[38;5;241m=\u001b[39m plan\u001b[38;5;241m.\u001b[39mextra_rows[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root_mask_term]\n\u001b[0;32m--> 389\u001b[0m root_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_root_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m dates, sids, root_mask_values \u001b[38;5;241m=\u001b[39m explode(root_mask)\n\u001b[1;32m    397\u001b[0m workspace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_populate_initial_workspace(\n\u001b[1;32m    398\u001b[0m     {\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root_mask_term: root_mask_values,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    405\u001b[0m     sids,\n\u001b[1;32m    406\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/Finance/lib/python3.9/site-packages/zipline/pipeline/engine.py:503\u001b[0m, in \u001b[0;36mSimplePipelineEngine._compute_root_mask\u001b[0;34m(self, domain, start_date, end_date, extra_rows)\u001b[0m\n\u001b[1;32m    500\u001b[0m num_assets \u001b[38;5;241m=\u001b[39m ret\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_assets \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find any assets with country_code \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m that traded \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetween \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    506\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis probably means that your asset db is old or that it has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincorrect country/exchange metadata.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    508\u001b[0m             domain\u001b[38;5;241m.\u001b[39mcountry_code,\n\u001b[1;32m    509\u001b[0m             start_date,\n\u001b[1;32m    510\u001b[0m             end_date,\n\u001b[1;32m    511\u001b[0m         )\n\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find any assets with country_code 'US' that traded between 2016-01-05 00:00:00 and 2016-01-06 00:00:00.\nThis probably means that your asset db is old or that it has incorrect country/exchange metadata."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the start and end dates\n",
    "start_date = pd.Timestamp('2016-01-05')\n",
    "end_date = pd.Timestamp('2016-01-06')\n",
    "\n",
    "\n",
    "# check if start_date is in sessions\n",
    "\n",
    "# all_sessions = pipeline.domain(default=US_EQUITIES).sessions().to_list()\n",
    "\n",
    "# session_in_2016 = [session for session in all_sessions if session >= start_date]\n",
    "# in the list of all sessions containing the Timestamps filter between start_date and end_date\n",
    "\n",
    "\n",
    "# Run our pipeline for the given start and end dates\n",
    "pipeline_output = engine.run_pipeline(pipeline, start_date, end_date)\n",
    "\n",
    "# We print information about the pipeline output\n",
    "print('The pipeline output has type:', type(pipeline_output), '\\n')\n",
    "\n",
    "# We print whether the pipeline output is a MultiIndex Dataframe\n",
    "print('Is the pipeline output a MultiIndex Dataframe:', isinstance(pipeline_output.index, pd.core.index.MultiIndex), '\\n')\n",
    "\n",
    "# If the pipeline output is a MultiIndex Dataframe we print the two levels of the index\n",
    "if isinstance(pipeline_output.index, pd.core.index.MultiIndex):\n",
    "\n",
    "    # We print the index level 0\n",
    "    print('Index Level 0:\\n\\n', pipeline_output.index.get_level_values(0), '\\n')\n",
    "\n",
    "    # We print the index level 1\n",
    "    print('Index Level 1:\\n\\n', pipeline_output.index.get_level_values(1), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Compute values for ``pipeline`` from ``start_date`` to ``end_date``.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "pipeline : zipline.pipeline.Pipeline\n",
      "    The pipeline to run.\n",
      "start_date : pd.Timestamp\n",
      "    Start date of the computed matrix.\n",
      "end_date : pd.Timestamp\n",
      "    End date of the computed matrix.\n",
      "hooks : list[implements(PipelineHooks)], optional\n",
      "    Hooks for instrumenting Pipeline execution.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "result : pd.DataFrame\n",
      "    A frame of computed results.\n",
      "\n",
      "    The ``result`` columns correspond to the entries of\n",
      "    `pipeline.columns`, which should be a dictionary mapping strings to\n",
      "    instances of :class:`zipline.pipeline.Term`.\n",
      "\n",
      "    For each date between ``start_date`` and ``end_date``, ``result``\n",
      "    will contain a row for each asset that passed `pipeline.screen`.\n",
      "    A screen of ``None`` indicates that a row should be returned for\n",
      "    each asset that existed each day.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/Finance/lib/python3.9/site-packages/zipline/pipeline/engine.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "engine.run_pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the return value of `.run_pipeline()` is a `MultiIndex` Pandas DataFrame containing a row for each asset that passed our pipeline’s screen. We can also see that the 0th level of the index contains the date and the 1st level of the index contains the tickers. In general, the returned Pandas DataFrame will also contain a column for each factor and filter we add to the pipeline using  `Pipeline.add()`. At this point we haven't added any factors or filters to our pipeline, consequently, the Pandas Dataframe will have no columns. In the following sections we will see how to add factors and filters to our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tickers\n",
    "\n",
    "We saw in the previous section, that the tickers of the stocks that passed our pipeline’s screen are contained in the 1st level of the index. Therefore, we can use the Pandas `.get_level_values(1).values.tolist()` method to get the tickers of those stocks and save them to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values in index level 1 and save them to a list\n",
    "universe_tickers = pipeline_output.index.get_level_values(1).values.tolist()\n",
    "\n",
    "# Display the tickers\n",
    "universe_tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data\n",
    "\n",
    "Now that we have the tickers for the stocks that passed our pipeline’s screen, we can get the historical stock data for those tickers from our data bundle. In order to get the historical data we need to use Zipline's `DataPortal` class. A `DataPortal` is an interface to all of the data that a Zipline simulation needs. In the code below, we will create a `DataPortal` and `get_pricing` function to get historical stock prices for our tickers. \n",
    "\n",
    "We have already seen most of the parameters used below when we create the `DataPortal`, so we won't explain them again here. The only new parameter is `first_trading_day`. The `first_trading_day` parameter is a `pd.Timestamp` indicating the first trading day for the simulation. We will set the first trading day to the first trading day in the data bundle. For more information on the `DataPortal` class see the [Zipline documentation](https://www.zipline.io/appendix.html?highlight=dataportal#zipline.data.data_portal.DataPortal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.data.data_portal import DataPortal\n",
    "\n",
    "# Create a data portal\n",
    "data_portal = DataPortal(bundle_data.asset_finder,\n",
    "                         trading_calendar = trading_calendar,\n",
    "                         first_trading_day = bundle_data.equity_daily_bar_reader.first_trading_day,\n",
    "                         equity_daily_reader = bundle_data.equity_daily_bar_reader,\n",
    "                         adjustment_reader = bundle_data.adjustment_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created a `data_portal` we will create a helper function, `get_pricing`, that gets the historical data from the `data_portal` for a given set of `start_date` and `end_date`. The `get_pricing` function takes various parameters: \n",
    "\n",
    "```python\n",
    "def get_pricing(data_portal, trading_calendar, assets, start_date, end_date, field='close')\n",
    "```\n",
    "\n",
    "\n",
    "The first two parameters, `data_portal` and `trading_calendar`, have already been defined above. The third paramter, `assets`, is a list of tickers. In our case we will use the tickers from the output of our pipeline, namely, `universe_tickers`. The fourth and fifth parameters are strings specifying the `start_date` and `end_date`. The function converts these two strings into Timestamps with a Custom Business Day frequency. The last parameter, `field`, is a string used to indicate which field to return. In our case we want to get the closing price, so we set `field='close`. \n",
    "\n",
    "The function returns the historical stock price data using the `.get_history_window()` attribute of the `DataPortal` class. This attribute returns a Pandas Dataframe containing the requested history window with the data fully adjusted. The `bar_count` parameter is an integer indicating the number of days to return. The number of days determines the number of rows of the returned dataframe. Both the `frequency` and `data_frequency` parameters are strings that indicate the frequency of the data to query, *i.e.* whether the data is in `daily` or `minute` intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pricing(data_portal, trading_calendar, assets, start_date, end_date, field='close'):\n",
    "    \n",
    "    # Set the given start and end dates to Timestamps. The frequency string C is used to\n",
    "    # indicate that a CustomBusinessDay DateOffset is used\n",
    "    end_dt = pd.Timestamp(end_date, tz='UTC', freq='C')\n",
    "    start_dt = pd.Timestamp(start_date, tz='UTC', freq='C')\n",
    "\n",
    "    # Get the locations of the start and end dates\n",
    "    end_loc = trading_calendar.closes.index.get_loc(end_dt)\n",
    "    start_loc = trading_calendar.closes.index.get_loc(start_dt)\n",
    "\n",
    "    # return the historical data for the given window\n",
    "    return data_portal.get_history_window(assets=assets, end_dt=end_dt, bar_count=end_loc - start_loc,\n",
    "                                          frequency='1d',\n",
    "                                          field=field,\n",
    "                                          data_frequency='daily')\n",
    "\n",
    "# Get the historical data for the given window\n",
    "historical_data = get_pricing(data_portal, trading_calendar, universe_tickers,\n",
    "                              start_date='2011-01-05', end_date='2016-01-05')\n",
    "# Display the historical data\n",
    "historical_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Alignment\n",
    "\n",
    "When pipeline returns with a date of, e.g., `2016-01-07` this includes data that would be known as of before the **market open** on `2016-01-07`. As such, if you ask for latest known values on each day, it will return the closing price from the day before and label the date `2016-01-07`. All factor values assume to be run prior to the open on the labeled day with data known before that point in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Factors and Filters\n",
    "\n",
    "Now that you know how build a pipeline and execute it, in this section we will see how we can add factors and filters to our pipeline. These factors and filters will determine the computations we want our pipeline to compute each day.\n",
    "\n",
    "We can add both factors and filters to our pipeline using the `.add(column, name)` method of the `Pipeline` class. The `column` parameter represetns the factor or filter to add to the pipeline. The `name` parameter is a string that determines the name of the column in the output Pandas Dataframe for that factor of fitler. As mentioned earlier, each factor and filter will appear as a column in the output dataframe of our pipeline. Let's start by adding a factor to our pipeline.\n",
    "\n",
    "### Factors\n",
    "\n",
    "In the code below, we will use Zipline's built-in `SimpleMovingAverage` factor to create a factor that computes the 15-day mean closing price of securities. We will then add this factor to our pipeline and use `.show_graph()` to see a diagram of our pipeline with the factor added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline.factors import SimpleMovingAverage\n",
    "\n",
    "# Create a factor that computes the 15-day mean closing price of securities\n",
    "mean_close_15 = SimpleMovingAverage(inputs = [USEquityPricing.close], window_length = 15)\n",
    "\n",
    "# Add the factor to our pipeline\n",
    "pipeline.add(mean_close_15, '15 Day MCP')\n",
    "\n",
    "# Render the pipeline as a DAG\n",
    "pipeline.show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the diagram above we can clearly see the factor we have added. Now, we can run our pipeline again and see its output. The pipeline is run in exactly the same way we did before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set starting and end dates\n",
    "start_date = pd.Timestamp('2014-01-06', tz='utc')\n",
    "end_date = pd.Timestamp('2016-01-05', tz='utc')\n",
    "\n",
    "# Run our pipeline for the given start and end dates\n",
    "output = engine.run_pipeline(pipeline, start_date, end_date)\n",
    "\n",
    "# Display the pipeline output\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now our output dataframe contains a column with the name `15 Day MCP`, which is the name we gave to our factor before. This ouput dataframe from our pipeline gives us the 15-day mean closing price of the securities that passed our `screen`.\n",
    "\n",
    "### Filters\n",
    "\n",
    "Filters are created and added to the pipeline in the same way as factors. In the code below, we create a filter that returns `True` whenever the 15-day average closing price is above \\$100. Remember, a filter produces a `True` or `False` value for each security every day. We will then add this filter to our pipeline and use `.show_graph()` to see a diagram of our pipeline with the filter added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Filter that returns True whenever the 15-day average closing price is above $100\n",
    "high_mean = mean_close_15 > 100\n",
    "\n",
    "# Add the filter to our pipeline\n",
    "pipeline.add(high_mean, 'High Mean')\n",
    "\n",
    "# Render the pipeline as a DAG\n",
    "pipeline.show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the diagram above we can clearly see the fiter we have added. Now, we can run our pipeline again and see its output. The pipeline is run in exactly the same way we did before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set starting and end dates\n",
    "start_date = pd.Timestamp('2014-01-06', tz='utc')\n",
    "end_date = pd.Timestamp('2016-01-05', tz='utc')\n",
    "\n",
    "# Run our pipeline for the given start and end dates\n",
    "output = engine.run_pipeline(pipeline, start_date, end_date)\n",
    "\n",
    "# Display the pipeline output\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now our output dataframe contains a two columns, one for the filter and one for the factor. The new column has the name `High Mean`, which is the name we gave to our filter before. Notice that the filter column only contains Boolean values, where only the securities with a 15-day average closing price above \\$100 have `True` values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
